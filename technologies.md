# Technologies used in our Waupaca Project

## I. For Data Cleaning, Processing, and Analysis

    Jupyter Notebook – we will use this open source web application to create and share documents that contain our code, 
    some visualizations, and narrative.

    Python – we will use python to read and extract data from our csv files

    Pandas - will be use to clean, process, and perform exploratory analysis on our dataset.
    
    Other Python libraries used:  Numpy, GeoPy, Nomanatium, Matplotlib

## II. Database Storage:

      Postgres db - is the database we will use to display the process data
      AWS

## III. Machine Learning models:

        SciKitLearn – will be use in our supervised learning model for classification, regression, clustering and dimensionality reduction.

        Matplotlib –  will be use for plotting and visualizing the data in various forms.

        TensorFlow –  we will use tensorflow to train and test our neural network model

        Numpy –  we will use numpy for our mathematical, logical, sorting and some basic statistical operation.

        Sklearn – we will use this tools to implement the Leanier Regression model.

## IV. Tableau Desktop
Tableau will be use to carry on further analysis on our dataset, which will include interactive visualization, stories of our dataset, and a dashboard.

## V . Time series model
        Seasonal decomposition - we used this in our model to seperate time series data into its core component; those include potential trend, seasonality,
        and the remaining random residual.
        
        ARIMA - we use this model to predict future point in the series and to also better understand the data.
        
        adfuller - The ADfuller test was use to detect whether a time series is stationary or not. 
